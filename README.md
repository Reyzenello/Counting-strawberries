# Counting-strawberries
Tokenizer experiments


All these attempts is to solve:

![image](https://github.com/Reyzenello/Counting-strawberries/assets/43668563/ce6d9afe-4bf3-489e-afb0-25230f9643e8)


Which btw all these could be easibly solved by using Prompt Engineering or just re-try so that attention mechanism will got fired. Usage of system 2 of Daniel Kaneman:

![image](https://github.com/Reyzenello/Counting-strawberries/assets/43668563/fde3fc5e-c413-4568-becc-de4c6e3e2749)


Prompt Engineering:

![image](https://github.com/Reyzenello/Counting-strawberries/assets/43668563/c4d1f70b-7ed8-4d00-809f-dee5b1b6b44e)

or if you want to complex your life:

![image](https://github.com/Reyzenello/Counting-strawberries/assets/43668563/32a8eb58-d0fc-4737-8b5b-a0d3d464fd03)


One of the reason why the tokenizer by word is not really effective is because of the bottleneck which I would call it "Library of Babel - Borghes".

I recommed to read that book, the result is it would be needed like yottabyte if we use the library of babel so yes is better the current tokenization instead of relying to word-per-word.

The only repo which I saw that is working by using defined vocabulary but in that sense it would be kinda cheating is (https://github.com/google/sentencepiece/blob/master/README.md)
